{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from math import inf\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =  gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state (array([-0.01303447,  0.02214191, -0.04855942,  0.00686143], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(f'Starting state {state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_network is running on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, state_dims):\n",
    "    super().__init__()\n",
    "    self.block = nn.Sequential(\n",
    "        nn.Linear(in_features = state_dims, out_features= 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features = 128, out_features= 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features = 64, out_features= 2)\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.block(x)\n",
    "\n",
    "q_network =  DQN(env.observation_space.shape[0]).to(device)\n",
    "print(f'q_network is running on {next(q_network.parameters()).device}')\n",
    "target_q_network = deepcopy(q_network).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1176, -0.1095], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dummy_tensor = torch.rand(4)\n",
    "print(q_network(dummy_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.can_sample(batch_size)\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        batch = zip(*batch)\n",
    "        return [torch.cat(items) for items in batch]\n",
    "\n",
    "    def can_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size * 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_policy(state, epsilon):\n",
    "  if random.random() < epsilon:\n",
    "    return torch.tensor(env.action_space.sample()).view(1,-1)\n",
    "  else:\n",
    "    tensor_state = torch.from_numpy(state).unsqueeze(dim = 0)\n",
    "    action_logits = q_network(tensor_state).detach()\n",
    "    return torch.argmax(action_logits, dim = 1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params= q_network.parameters(),\n",
    "                             lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(state, reward, done, next_state):\n",
    "  tensor_state = torch.from_numpy(state).unsqueeze(dim=0)\n",
    "  tensor_next_state = torch.from_numpy(next_state).unsqueeze(dim = 0)\n",
    "  tensor_reward = torch.tensor(reward).view(1,-1)\n",
    "  tensor_done = torch.tensor(done).view(1, -1)\n",
    "  return tensor_state, tensor_next_state, tensor_reward, tensor_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(episodes, batch_size=32, gamma=0.5, initial_epsilon=1, decay_rate = 0.01):\n",
    "  # stats = {'MSE Loss': [], 'Returns': []}\n",
    "  memory = ReplayBuffer()\n",
    "\n",
    "  for episode in tqdm(range(episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    epsilon = initial_epsilon * math.e ** (-decay_rate * episode)\n",
    "    ep_return = 0\n",
    "    while not done:\n",
    "      tensor_action = exploratory_policy(state, epsilon)\n",
    "      next_state, reward, done, _ = env.step(tensor_action.item())\n",
    "      tensor_state, tensor_next_state, tensor_reward = preprocess(state, reward, done, next_state)\n",
    "      memory.insert([tensor_state, tensor_action, tensor_reward, tensor_done, tensor_next_state])\n",
    "\n",
    "      if memory.can_sample(batch_size):\n",
    "        state_b, action_b, reward_b, done_b, next_state_b = memory.sample(batch_size)\n",
    "        qsa_b = q_network(state_b).gather(1, action_b)\n",
    "        next_qsa_b = torch.max(target_q_network(next_state_b), dim = -1, keepdim = True)[0]\n",
    "\n",
    "        target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
    "        loss = loss_fn(qsa_b, target_b)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('MSE Loss', loss.item(), episode)\n",
    "        # stats['MSE Loss'].append(loss.item())\n",
    "\n",
    "      state = next_state\n",
    "      ep_return += reward\n",
    "\n",
    "    writer.add_scalar('Episode Return', ep_return, episode)\n",
    "    # stats['Returns'].append(ep_return)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "  writer.close()\n",
    "  return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2c3b305b3842d3b2714ff5dbc5998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaiser/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m deep_q_learning(\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mdeep_q_learning\u001b[0;34m(episodes, batch_size, gamma, initial_epsilon, decay_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     11\u001b[0m   tensor_action \u001b[38;5;241m=\u001b[39m exploratory_policy(state, epsilon)\n\u001b[0;32m---> 12\u001b[0m   next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(tensor_action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m   tensor_state, tensor_next_state, tensor_reward \u001b[38;5;241m=\u001b[39m preprocess(state, reward, done, next_state)\n\u001b[1;32m     14\u001b[0m   memory\u001b[38;5;241m.\u001b[39minsert([tensor_state, tensor_action, tensor_reward, tensor_done, tensor_next_state])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "stats = deep_q_learning(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
